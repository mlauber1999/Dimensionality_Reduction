{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrop05Okknt8"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4apgYbQenymW"
      },
      "outputs": [],
      "source": [
        "# !pip install xgboost lightgbm catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmIJYSsImkOZ"
      },
      "outputs": [],
      "source": [
        "# !pip install minisom #allows you to implement SOM in python/pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVdx8rsEhXW2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "# import timm # library with pretrained models\n",
        "from datasets import load_dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from minisom import MiniSom\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import warnings\n",
        "from sklearn.neural_network import BernoulliRBM #built in sklearn RBM with bernoulli units (both visible and hidden units are binary, so need to make sure data is normalized)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script contains code to perform dimensionality reduction techniques:\n",
        "1. Self Organizing Map \n",
        "2. Restricted Bolzman Machine \n",
        "3. Variational Auto Encoder "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U3VXPo-7htj7"
      },
      "source": [
        "\n",
        "- Fashion mnist data set\n",
        "- each image is 28x28 pixels (784 pixels total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uONs2lyje1J"
      },
      "outputs": [],
      "source": [
        "#define normalization transforms so that data is between 0 and 1 (required for RBM and VAE)\n",
        "#images are grayscale, they have a single channel, so normalize them with mean and standard deviation to scale pixel values between 0 and 1\n",
        "transform = transforms.Compose([transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIX-iaAilFUc"
      },
      "outputs": [],
      "source": [
        "#load train set and test set of data\n",
        "#these have been transformed and normalized so can just directly call these variables for the different dim reduction techniques\n",
        "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7Pe250sjnNA"
      },
      "outputs": [],
      "source": [
        "print(train_set[0]) #data now between 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC8UbvdIhycu"
      },
      "source": [
        "1) Apply SOM, RBM, and one arbitrary Autoencoders (e.g. VAE) to it to reduce the dimension of the original data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjPhfWWDh1U1"
      },
      "source": [
        "2) For each dim reduction technique\n",
        "- Experiment with three classifier algorithms (XGBoost, LightGBM and CATBoost) and run them on the four datasets.\n",
        "- Three dataset includes the original dataset, SOM, RBM, and one Autoencoder.\n",
        "- Then compare the results. For classification comparison, you can use accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2KA-yeQzPM7"
      },
      "source": [
        "First do each classifier on the original dataset (un-reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keN17R2K0fpF"
      },
      "outputs": [],
      "source": [
        "#first need to flatten data since images are 28x28\n",
        "#this flattens them into 784 dimensional vectors\n",
        "def flatten_data(data_loader):\n",
        "    flattened_data = []\n",
        "    for images, labels in data_loader:\n",
        "        for image in images:\n",
        "            flattened_data.append(image.numpy().ravel())\n",
        "    return np.array(flattened_data)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
        "\n",
        "flattened_train_data = flatten_data(train_loader)\n",
        "flattened_test_data = flatten_data(test_loader)\n",
        "\n",
        "# Extract labels\n",
        "train_labels = train_set.targets.numpy()\n",
        "test_labels = test_set.targets.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mmEyvlpz008",
        "outputId": "8f335054-6839-4a4e-aa84-7b09034efaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Data XGB run time: 212.8142101764679 seconds\n",
            "Original Data XGBoost Accuracy: 0.8768333333333334\n"
          ]
        }
      ],
      "source": [
        "#XGB on original data\n",
        "# split the flattened data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(flattened_train_data, train_labels, test_size=0.2)\n",
        "\n",
        "og_xgb_start = time.time()\n",
        "#was taking forever to train, so potentially adjust the hyper paramters\n",
        "\n",
        "# initialize and train xgb\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=50,max_depth=3) #taking forever so lower number of estimators from 100 to 50, reduce depth to 3\n",
        "xgb_model.fit(X_train, Y_train)\n",
        "og_xgb_accuracy = xgb_model.score(X_test, Y_test)\n",
        "og_xgb_end = time.time()\n",
        "og_xgb_run = og_xgb_end - og_xgb_start\n",
        "print(f\"Original Data XGB run time: {og_xgb_run} seconds\")\n",
        "print(f\"Original Data XGBoost Accuracy: {og_xgb_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjcTpLWAz3Qd",
        "outputId": "cb91f257-bfc0-4cd8-9922-169d2f87bbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Data LGBM run time: 91.79607462882996 seconds\n",
            "Original Data LightGBM Accuracy: 0.8630833333333333\n"
          ]
        }
      ],
      "source": [
        "#LightGBM on original data\n",
        "#tweaked params and now it runs in 5 minutes (ish)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(flattened_train_data, train_labels, test_size=0.2)\n",
        "\n",
        "og_lgb_start = time.time()\n",
        "og_lgb_model = lgb.LGBMClassifier(verbose=-1,n_estimators=50,max_depth=3,subsample=.5) # verbose = -1 should suppress the flood of warnings\n",
        "og_lgb_model.fit(X_train, Y_train)\n",
        "og_lgb_accuracy = og_lgb_model.score(X_test, Y_test)\n",
        "og_lgb_end = time.time()\n",
        "og_lgb_run = og_lgb_end - og_lgb_start\n",
        "#print accuracy and duration\n",
        "print(f\"Original Data LGBM run time: {og_lgb_run} seconds\")\n",
        "print(f\"Original Data LightGBM Accuracy: {og_lgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk-2g-psz9TY",
        "outputId": "0a74fe0c-3311-44b8-c80e-8dec9cb0aba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data catboost run time: 71.28506112098694 seconds\n",
            " Original Data CatBoost Accuracy: 0.8323333333333334\n"
          ]
        }
      ],
      "source": [
        "#CatBoost on original data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(flattened_train_data, train_labels, test_size=0.2)\n",
        "\n",
        "og_cb_start = time.time()\n",
        "cb_model = cb.CatBoostClassifier(verbose=0, n_estimators=50,max_depth=3) #doesnt support subsample param\n",
        "cb_model.fit(X_train, Y_train)\n",
        "og_cb_accuracy = cb_model.score(X_test, Y_test)\n",
        "og_cb_end = time.time()\n",
        "og_cb_run = og_cb_end - og_cb_start\n",
        "print(f\"Original data catboost run time: {og_cb_run} seconds\")\n",
        "print(f\" Original Data CatBoost Accuracy: {og_cb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr8rNODQOm7N",
        "outputId": "d01df117-f069-46c7-84a2-001ea5f33bd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    OG Model  Accuracy    Duration\n",
            "0    XGBoost  0.876833  212.814210\n",
            "1  Light GBM  0.863083   91.796075\n",
            "2   CatBoost  0.832333   71.285061\n"
          ]
        }
      ],
      "source": [
        "#create df for original dataset models and accuracy\n",
        "og_dic = {\"OG Model\":[\"XGBoost\",'Light GBM',\"CatBoost\"],\"Accuracy\":[og_xgb_accuracy,og_lgb_accuracy,og_cb_accuracy],\"Duration\":[og_xgb_run,og_lgb_run,og_cb_run]}\n",
        "rbm_df = pd.DataFrame(og_dic)\n",
        "print(rbm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM93W03fjNlP"
      },
      "source": [
        "Self organizing map (SOM)\n",
        "- used to reduce dimensionality of data\n",
        "- output of som is a topographical map which represnts data in lower dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA1Iy4bwo4O-",
        "outputId": "8c44aff7-be84-4213-f8f2-38f2fe9369f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "som data transform training duration 5.172396183013916\n"
          ]
        }
      ],
      "source": [
        "# initialize and train the SOM\n",
        "#miniSom pretrained model\n",
        "som = MiniSom(6, 6, 784, sigma=0.3, learning_rate=0.5)\n",
        "som.train(flattened_train_data, num_iteration=100)\n",
        "\n",
        "# now use SOM to transform data\n",
        "#time the SOM\n",
        "som_transform_start = time.time()\n",
        "def transform_with_som(som, data):\n",
        "    transformed_data = [som.winner(vec) for vec in data]\n",
        "    return np.array(transformed_data)\n",
        "\n",
        "som_x_train_transformed = transform_with_som(som, flattened_train_data)\n",
        "som_x_test_transformed = transform_with_som(som, flattened_test_data)\n",
        "som_transform_end = time.time()\n",
        "som_transform_run = som_transform_end - som_transform_start\n",
        "print(f\"som data transform training duration {som_transform_run}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ4DBtLNo_jE",
        "outputId": "4813fde1-4077-4421-9801-e755943484e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SOM XGB run time: 6.8260111808776855 seconds\n",
            "XGBoost Accuracy: 0.329\n"
          ]
        }
      ],
      "source": [
        "# split som transformed data for XGB model\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(som_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "som_xgb_start = time.time()\n",
        "# initialize and train xgb\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train, Y_train)\n",
        "som_xgb_accuracy = xgb_model.score(X_test, Y_test)\n",
        "som_xgb_end = time.time()\n",
        "som_xgb_run = som_xgb_end - som_xgb_start\n",
        "#print accuracy and duration\n",
        "print(f\"SOM XGB run time: {som_xgb_run} seconds\")\n",
        "print(f\"XGBoost Accuracy: {som_xgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vhTcjG5pj56",
        "outputId": "9cba3802-4c7d-473f-a22f-cf64153af8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SOM LGBM run time: 4.239447593688965 seconds\n",
            "LightGBM Accuracy: 0.32508333333333334\n"
          ]
        }
      ],
      "source": [
        "#lgb on som\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(som_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "som_lgb_start = time.time()\n",
        "lgb_model = lgb.LGBMClassifier(verbose=-1) # verbose = -1 should suppress warnings once it's done\n",
        "lgb_model.fit(X_train, Y_train)\n",
        "som_lgb_accuracy = lgb_model.score(X_test, Y_test)\n",
        "som_lgb_end = time.time()\n",
        "som_lgb_run = som_lgb_end - som_lgb_start\n",
        "#print accuracy and duration\n",
        "print(f\"SOM LGBM run time: {som_lgb_run} seconds\")\n",
        "print(f\"LightGBM Accuracy: {som_lgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-1RWySTpwmH",
        "outputId": "aa497214-846a-4dac-bcb6-0fea65d7f1e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SOM catboost run time: 64.07290768623352 seconds\n",
            "CatBoost Accuracy: 0.32675\n"
          ]
        }
      ],
      "source": [
        "#catboost on som\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(som_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "som_cb_start = time.time()\n",
        "cb_model = cb.CatBoostClassifier(verbose=0)\n",
        "cb_model.fit(X_train, Y_train)\n",
        "som_cb_accuracy = cb_model.score(X_test, Y_test)\n",
        "som_cb_end = time.time()\n",
        "som_cb_run = som_cb_end - som_cb_start\n",
        "print(f\"SOM catboost run time: {som_cb_run} seconds\")\n",
        "print(f\"CatBoost Accuracy: {som_cb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__12DrLBnxee",
        "outputId": "84eee558-6655-4f8b-d41e-884d66caf41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SOM Model  Accuracy   Duration\n",
            "0    XGBoost  0.329000   6.826011\n",
            "1  Light GBM  0.325083   4.239448\n",
            "2   CatBoost  0.326750  64.072908\n"
          ]
        }
      ],
      "source": [
        "#store som model accuraccies in dictionary\n",
        "som_dic = {\"SOM Model\":[\"XGBoost\",'Light GBM',\"CatBoost\"],\"Accuracy\":[som_xgb_accuracy,som_lgb_accuracy,som_cb_accuracy],\"Duration\":[som_xgb_run,som_lgb_run,som_cb_run]}\n",
        "som_df = pd.DataFrame(som_dic)\n",
        "print(som_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPcIWK4YllB4"
      },
      "source": [
        "Restricted Bolzman Machine (RBM)\n",
        "- no connection between neruons of the same layer\n",
        "- but there are connections from each invisible neuron to each hidden neuron and vice versa\n",
        "- used sklearn built in bernoulli RBM, menaing both visbile and hidden units are binary (have states of 0 or 1). Use it on binary or normalized data (I need to make sure the data transform includes normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-T9Kd06mcYC",
        "outputId": "d72fc72a-c924-459d-d5d3-6ae4e3a29d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -251.85, time = 12.87s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -237.06, time = 13.97s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -224.46, time = 13.42s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -216.23, time = 11.55s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -208.86, time = 13.41s\n",
            "rbm data transform training duration 66.23604893684387\n"
          ]
        }
      ],
      "source": [
        "#initialize bernoulli rbm on normalized/transformed data now between 0 and 1\n",
        "#this one takes a minute to train (time it)\n",
        "rbm_transform_start = time.time()\n",
        "rbm = BernoulliRBM(n_components=100, learning_rate=0.01, n_iter=5, verbose=True)\n",
        "#train rbm\n",
        "rbm.fit(flattened_train_data)\n",
        "rbm_transform_end = time.time()\n",
        "rbm_transform_run = rbm_transform_end - rbm_transform_start\n",
        "print(f\"rbm data transform training duration {rbm_transform_run}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyAzOylftxWR"
      },
      "outputs": [],
      "source": [
        "print(flattened_train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak5zgMP_tCO2"
      },
      "outputs": [],
      "source": [
        "#dim reduction of data using trained RBM\n",
        "rbm_x_train_transformed = rbm.transform(flattened_train_data)\n",
        "rbm_x_test_transformed = rbm.transform(flattened_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a3OIxDAtro6",
        "outputId": "11bf79ad-ef0f-4003-d8d7-13610c0852f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RBM XGB run time: 67.01226925849915 seconds\n",
            "RBM XGBoost Accuracy: 0.8704166666666666\n"
          ]
        }
      ],
      "source": [
        "#implement xgb on rbm transformed data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(rbm_x_train_transformed, train_labels, test_size=.2)\n",
        "\n",
        "rbm_xgb_start = time.time()\n",
        "# initialize and train xgb on rbm data\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train, Y_train)\n",
        "rbm_xgb_accuracy = xgb_model.score(X_test, Y_test)\n",
        "rbm_xgb_end = time.time()\n",
        "rbm_xgb_run = rbm_xgb_end - rbm_xgb_start\n",
        "#print accuracy and duration\n",
        "print(f\"RBM XGB run time: {rbm_xgb_run} seconds\")\n",
        "print(f\"RBM XGBoost Accuracy: {rbm_xgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOKdBQ2TxY3C",
        "outputId": "6f3ed79e-7e95-4828-a2df-d63f3ced10a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RBM LGBM run time: 50.01668334007263 seconds\n",
            "RBM LightGBM Accuracy: 0.8630833333333333\n"
          ]
        }
      ],
      "source": [
        "#lightgbm on rbm data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(rbm_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "rbm_lgb_start = time.time()\n",
        "lgb_model = lgb.LGBMClassifier(verbose=-1) # verbose = -1 should suppress the flood of warnings\n",
        "lgb_model.fit(X_train, Y_train)\n",
        "rbm_lgb_accuracy = lgb_model.score(X_test, Y_test)\n",
        "rbm_lgb_end = time.time()\n",
        "rbm_lgb_run = rbm_lgb_end - rbm_lgb_start\n",
        "print(f\"RBM LGBM run time: {rbm_lgb_run} seconds\")\n",
        "print(f\"RBM LightGBM Accuracy: {rbm_lgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_e52__zxu8j",
        "outputId": "bade8f53-818c-49c0-9511-d4d1c6e078c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RBM catboost run time: 995.4395568370819 seconds\n",
            "RBM CatBoost Accuracy: 0.8709166666666667\n"
          ]
        }
      ],
      "source": [
        "#Catboost on rbm data\n",
        "#just from running I can tell this will probably be the model with longest duration\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(rbm_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "#start\n",
        "rbm_cb_start = time.time()\n",
        "cb_model = cb.CatBoostClassifier(verbose=0)\n",
        "cb_model.fit(X_train, Y_train)\n",
        "rbm_cb_accuracy = cb_model.score(X_test, Y_test)\n",
        "rbm_cb_end = time.time()\n",
        "rbm_cb_run = rbm_cb_end - rbm_cb_start\n",
        "#print accuracy and duration\n",
        "print(f\"RBM catboost run time: {rbm_cb_run} seconds\")\n",
        "print(f\"RBM CatBoost Accuracy: {rbm_cb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtMWmhp0yBBA",
        "outputId": "4c08756a-6dad-4de6-b2e3-ce3cc027b1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RBM Model  Accuracy    Duration\n",
            "0    XGBoost  0.870417   67.012269\n",
            "1  Light GBM  0.863083   50.016683\n",
            "2   CatBoost  0.870917  995.439557\n"
          ]
        }
      ],
      "source": [
        "#store rbm accuraccies in dictionary\n",
        "rbm_dic = {\"RBM Model\":[\"XGBoost\",'Light GBM',\"CatBoost\"],\"Accuracy\":[rbm_xgb_accuracy,rbm_lgb_accuracy,rbm_cb_accuracy],\"Duration\":[rbm_xgb_run,rbm_lgb_run,rbm_cb_run]}\n",
        "rbm_df = pd.DataFrame(rbm_dic)\n",
        "print(rbm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVUglWetl2s4"
      },
      "source": [
        "Variational Auto Enconder (VAE)\n",
        " - takes distributions of the orginal dataset and reconstruct the dataset from these distributions (instead of directly from input data)\n",
        " - does not assign a single value to each feature, rather assigns a probability\n",
        " - good for high dimensional or noisy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc5GZ69wmc0-"
      },
      "outputs": [],
      "source": [
        "#define VAE class\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # encoder part to compress data into embedding (latent representation)\n",
        "        self.fc1 = nn.Linear(784, 400) #transforms input vector of 784 to latent/hidden layer size of 400\n",
        "        self.fc21 = nn.Linear(400, 20)  #outputs the mean vector of the latent distribution\n",
        "        self.fc22 = nn.Linear(400, 20)  #outputs the log variance vector of latent distribution\n",
        "\n",
        "        # decoder part to reconstruct input data from latent representation\n",
        "        #these fully connected layers that upscale the data from the 20 dim latent vector back to the orignal 784 input size\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      #applies the first part of the encoder and uses that output to compute mean and log variance vectors\n",
        "        h1 = torch.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "      #performs reparamaterization trick so model can backpropogate through random samples (from latent distribution)\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "      #applies the decoder part of the VAE to reconstruct input data from latent representation\n",
        "        h3 = torch.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3)) #makes sure values are range of 0 to 1\n",
        "\n",
        "    #forward pass\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsA2m58xUq-0",
        "outputId": "fdec1ee7-a5d6-484f-c0bf-0b142eb58648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vae data transform training duration 828.7021872997284\n"
          ]
        }
      ],
      "source": [
        "#train the vae\n",
        "#this one took awhile to train\n",
        "# define loss function\n",
        "num_epochs = 1\n",
        "def vae_loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = torch.nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "vae_transform_start = time.time()\n",
        "# initialize the vae and optimizer\n",
        "vae = VAE()\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "# define training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        loss = vae_loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "vae_transform_end = time.time()\n",
        "vae_transform_run = vae_transform_end - vae_transform_start\n",
        "print(f\"vae data transform training duration {vae_transform_run}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq9ImShTUuvd"
      },
      "outputs": [],
      "source": [
        "#now use encoder the portion of vae to trasnform data\n",
        "def transform_with_vae(vae, data_loader):\n",
        "    vae.eval()\n",
        "    transformed_data = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in data_loader:\n",
        "            mu, logvar = vae.encode(data.view(-1, 784))\n",
        "            z = vae.reparameterize(mu, logvar)\n",
        "            transformed_data.extend(z.numpy())\n",
        "    return np.array(transformed_data)\n",
        "\n",
        "vae_x_train_transformed = transform_with_vae(vae, train_loader)\n",
        "vae_x_test_transformed = transform_with_vae(vae, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKCAjNCtVNER",
        "outputId": "09c64de0-56da-465b-9b81-a0238931b43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAE XGB run time: 13.416758298873901 seconds\n",
            "VAE XGBoost Accuracy: 0.7863333333333333\n"
          ]
        }
      ],
      "source": [
        "#now input vae transformed data into xgb\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(vae_x_train_transformed, train_labels, test_size=.2)\n",
        "\n",
        "vae_xgb_start = time.time()\n",
        "# initialize and train xgb on vae transformed data\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train, Y_train)\n",
        "vae_xgb_accuracy = xgb_model.score(X_test, Y_test)\n",
        "vae_xgb_end = time.time()\n",
        "vae_xgb_run = vae_xgb_end - vae_xgb_start\n",
        "# Print accuracy scores\n",
        "print(f\"VAE XGB run time: {vae_xgb_run} seconds\")\n",
        "print(f\"VAE XGBoost Accuracy: {vae_xgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntwceu7FVRO1",
        "outputId": "872f0258-a2b8-4642-a359-6f8249c2df82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAE LGBM run time: 11.01946234703064 seconds\n",
            "VAE LightGBM Accuracy: 0.7949166666666667\n"
          ]
        }
      ],
      "source": [
        "#vae lightgbm\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(vae_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "vae_lgb_start = time.time()\n",
        "# initialize and train lgb on vae transformed data\n",
        "lgb_model = lgb.LGBMClassifier(verbose=-1) # verbose = -1 should suppress the flood of warnings\n",
        "lgb_model.fit(X_train, Y_train)\n",
        "vae_lgb_accuracy = lgb_model.score(X_test, Y_test)\n",
        "vae_lgb_end = time.time()\n",
        "vae_lgb_run = vae_lgb_end - vae_lgb_start\n",
        "print(f\"VAE LGBM run time: {vae_lgb_run} seconds\")\n",
        "print(f\"VAE LightGBM Accuracy: {vae_lgb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23NRoDV6VSoz",
        "outputId": "3302530e-0b82-4b4d-a420-1effa2b08344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAE catboost run time: 203.94560265541077 seconds\n",
            "VAE CatBoost Accuracy: 0.7929166666666667\n"
          ]
        }
      ],
      "source": [
        "#vae catboost\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(vae_x_train_transformed, train_labels, test_size=0.2)\n",
        "\n",
        "vae_cb_start = time.time()\n",
        "# initialize and train catboost on vae transformed data\n",
        "cb_model = cb.CatBoostClassifier(verbose=0)\n",
        "cb_model.fit(X_train, Y_train)\n",
        "vae_cb_accuracy = cb_model.score(X_test, Y_test)\n",
        "vae_cb_end = time.time()\n",
        "vae_cb_run = vae_cb_end - vae_cb_start\n",
        "print(f\"VAE catboost run time: {vae_cb_run} seconds\")\n",
        "print(f\"VAE CatBoost Accuracy: {vae_cb_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOcuCa2JXiuG",
        "outputId": "b9adaff0-b6e1-4f86-fa1e-ec8f075785a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   VAE Model  Accuracy    Duration\n",
            "0    XGBoost  0.786333   13.416758\n",
            "1  Light GBM  0.794917   11.019462\n",
            "2   CatBoost  0.792917  203.945603\n"
          ]
        }
      ],
      "source": [
        "#create vae dic\n",
        "vae_dic = {\"VAE Model\":[\"XGBoost\",'Light GBM',\"CatBoost\"],\"Accuracy\":[vae_xgb_accuracy,vae_lgb_accuracy,vae_cb_accuracy],\"Duration\":[vae_xgb_run,vae_lgb_run,vae_cb_run]}\n",
        "vae_df = pd.DataFrame(vae_dic)\n",
        "print(vae_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhBKuGbeZvh6",
        "outputId": "b2f55133-60c3-457d-8397-c9ade6a00557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Model  Training Duration\n",
            "0   SOM           5.172396\n",
            "1   RBM          66.236049\n",
            "2   VAE         828.702187\n"
          ]
        }
      ],
      "source": [
        "#create dic for all the dim reduction model training duration\n",
        "training_dic = {\"Model\":['SOM',\"RBM\",\"VAE\"],\"Training Duration\":[som_transform_run,rbm_transform_run,vae_transform_run]}\n",
        "training_df = pd.DataFrame(training_dic)\n",
        "print(training_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
